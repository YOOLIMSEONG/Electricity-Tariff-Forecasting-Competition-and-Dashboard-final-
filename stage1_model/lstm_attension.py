# ==========================================================
# âš¡ stage1_model_train_autolog.py
# âœ… PyTorch + MLflow Autolog + Git/GPU ì¶”ì  (Stage1 ì „ì²˜ë¦¬ëª¨ë¸)
# ==========================================================
import os, json, random, subprocess, warnings
warnings.filterwarnings("ignore")
import re

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error
import optuna, joblib, mlflow
import matplotlib.pyplot as plt

# ==========================================================
# ê¸°ë³¸ ì„¤ì •
# ==========================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "../data")
SAVE_ROOT = "stage1_model"          # íƒ€ê¹ƒë³„ model_1, model_2 ...ê°€ ìƒì„±ë  ë£¨íŠ¸
os.makedirs(SAVE_ROOT, exist_ok=True)

train_path = os.path.join(DATA_DIR, "fixed_train_clean_v2.csv")
test_path  = os.path.join(DATA_DIR, "fixed_test_weather_full.csv")

TARGETS = [
    "ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)",
    "ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
    "ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
    "ì§€ìƒì—­ë¥ (%)",
    "ì§„ìƒì—­ë¥ (%)",
]

N_SPLITS = 3
N_TRIALS = 5
SEED = 42
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

torch.manual_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)

# ==========================================================
# ğŸ” Git / MLflow ì„¤ì •
# ==========================================================
def get_git_commit():
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"], stderr=subprocess.STDOUT).decode().strip()
    except Exception:
        return "unknown"

os.environ["GIT_PYTHON_REFRESH"] = "quiet"  # Git ê²½ê³  ì–µì œ
mlflow.set_experiment("Stage1_LSTM_Attention")
mlflow.autolog(log_models=True)              # âœ… í•œ ë²ˆë§Œ í˜¸ì¶œ

# ==========================================================
# Dataset
# ==========================================================
class SeqDataset(Dataset):
    def __init__(self, X, y, seq_len):
        self.X, self.y, self.seq_len = X, y, seq_len
    def __len__(self):
        return len(self.X) - self.seq_len
    def __getitem__(self, idx):
        return (
            torch.tensor(self.X[idx:idx+self.seq_len], dtype=torch.float32),
            torch.tensor(self.y[idx+self.seq_len], dtype=torch.float32)
        )

# ==========================================================
# LSTM + Attention ëª¨ë¸
# ==========================================================
class LSTMAttention(nn.Module):
    def __init__(self, input_dim, params):
        super().__init__()
        hidden  = params["units"]
        layers  = params["n_layers"]
        dropout = params["dropout"]
        n_heads = params.get("n_heads", 4)

        self.lstm = nn.LSTM(input_dim, hidden, num_layers=layers, batch_first=True, dropout=dropout)
        self.attn = nn.MultiheadAttention(embed_dim=hidden, num_heads=n_heads, dropout=dropout, batch_first=True)
        self.norm = nn.LayerNorm(hidden)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        attn_out, _ = self.attn(lstm_out, lstm_out, lstm_out)
        out = self.norm(attn_out + lstm_out)
        out = self.dropout(out)
        out = out.mean(dim=1)
        return self.fc(out).squeeze(-1)

# ==========================================================
# ì „ì²˜ë¦¬
# ==========================================================
def preprocess_dataframe(df):
    df = df.copy()
    for col in ["ì¸¡ì •ì¼ì‹œ_x", "ì¸¡ì •ì¼ì‹œ_y"]:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors="coerce")
            df = df.sort_values(col).reset_index(drop=True)
            break

    drop_cols = ["id", "ì¸¡ì •ì¼ì‹œ_x", "ì¸¡ì •ì¼ì‹œ_y"]
    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")

    if "season" in df.columns:
        df["season"] = df["season"].map({"spring":0,"summer":1,"autumn":2,"fall":2,"winter":3}).fillna(3).astype(int)
        df = pd.get_dummies(df, columns=["season"], prefix="season")

    if "ì‘ì—…ìœ í˜•" in df.columns:
        df = pd.get_dummies(df, columns=["ì‘ì—…ìœ í˜•"], prefix="ì‘ì—…ìœ í˜•")

    if "ë‚ ì”¨ì½”ë“œ" in df.columns:
        le = LabelEncoder()
        df["ë‚ ì”¨ì½”ë“œ"] = le.fit_transform(df["ë‚ ì”¨ì½”ë“œ"].astype(str))

    for col in df.columns:
        if df[col].dtype == "object":
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col].astype(str))
    return df

# ==========================================================
# í•™ìŠµ ë£¨í”„ + ê·¸ë˜í”„ ì €ì¥/ì—…ë¡œë“œ
# ==========================================================
def train_one_fold(model, train_loader, val_loader, optimizer, scheduler, criterion, epochs, fold_name, save_dir):
    best_loss = np.inf
    train_losses, val_losses, val_maes = [], [], []

    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        for Xb, yb in train_loader:
            Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
            optimizer.zero_grad()
            preds = model(Xb)
            loss = criterion(preds, yb)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * len(Xb)
        train_loss /= len(train_loader.dataset)

        model.eval()
        val_loss, preds, trues = 0.0, [], []
        with torch.no_grad():
            for Xb, yb in val_loader:
                Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
                y_pred = model(Xb)
                loss = criterion(y_pred, yb)
                val_loss += loss.item() * len(Xb)
                preds.append(y_pred.cpu().numpy())
                trues.append(yb.cpu().numpy())
        val_loss /= len(val_loader.dataset)
        scheduler.step(val_loss)

        # epochë³„ MAE (ìŠ¤ì¼€ì¼ëœ ê³µê°„) â€“ ì¶”ì„¸ ëª¨ë‹ˆí„°ë§ìš©
        y_pred = np.concatenate(preds)
        y_true = np.concatenate(trues)
        val_mae = mean_absolute_error(y_true, y_pred)

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_maes.append(val_mae)

        # MLflow ë©”íŠ¸ë¦­
        mlflow.log_metrics({
            f"{fold_name}_train_loss": train_loss,
            f"{fold_name}_val_loss":   val_loss,
            f"{fold_name}_val_mae":    val_mae
        }, step=epoch)

        # ê³¡ì„  ì €ì¥ + MLflow ì—…ë¡œë“œ
        plt.figure(figsize=(6,4))
        plt.plot(val_maes, label="Validation MAE", marker="o")
        plt.xlabel("Epoch"); plt.ylabel("MAE"); plt.title(f"{fold_name} (Epoch {epoch+1}/{epochs})")
        plt.grid(True); plt.legend(); plt.tight_layout()
        img_path = os.path.join(save_dir, f"{fold_name}_mae_curve.png")
        plt.savefig(img_path); plt.close()
        mlflow.log_artifact(img_path)

        # best ê°±ì‹ 
        if val_loss < best_loss:
            best_loss = val_loss
            best_state = model.state_dict().copy()

    model.load_state_dict(best_state)
    return best_loss, train_losses, val_losses

# ==========================================================
# âš™ï¸ Optuna Objective (mlflow + nested run)
# ==========================================================
def make_objective(target):
    X_full = train[feature_pool].values.astype(float)
    y_full = train[target].values.astype(float)

    def objective(trial):
        params = {
            "n_layers":  trial.suggest_int("n_layers", 1, 2),
            "units":     trial.suggest_int("units", 32, 128, step=32),
            "dropout":   trial.suggest_float("dropout", 0.1, 0.4, step=0.1),
            "n_heads":   trial.suggest_categorical("n_heads", [2, 4, 8]),
            "seq_len":   trial.suggest_int("seq_len", 12, 48, step=12),
            "lr":        trial.suggest_float("lr", 1e-4, 3e-3, log=True),
            "batch_size":trial.suggest_categorical("batch_size", [16, 32]),
            "epochs":    trial.suggest_int("epochs", 20, 40, step=10),
        }

        tscv = TimeSeriesSplit(n_splits=N_SPLITS)
        fold_mae = []
        fold_results = {}

        # íƒ€ê¹ƒë³„ ì €ì¥ í´ë” (ì˜ˆ: stage1_model/model_1)
        model_dir = os.path.join(SAVE_ROOT, f"model_{TARGETS.index(target)+1}")
        os.makedirs(model_dir, exist_ok=True)

        # âœ… nested runìœ¼ë¡œ ì¶©ëŒ ë°©ì§€
        with mlflow.start_run(run_name=f"{target}_trial", nested=True):
            mlflow.log_params(params)

            for fold, (tr_idx, va_idx) in enumerate(tscv.split(X_full)):
                fold_name = f"fold{fold+1}"
                x_scaler = StandardScaler(); y_scaler = StandardScaler()

                X_tr = x_scaler.fit_transform(X_full[tr_idx])
                X_va = x_scaler.transform(X_full[va_idx])
                y_tr = y_full[tr_idx].reshape(-1,1); y_va = y_full[va_idx].reshape(-1,1)
                y_tr_s = y_scaler.fit_transform(y_tr).ravel()
                y_va_s = y_scaler.transform(y_va).ravel()

                seq_len = min(params["seq_len"], max(3, len(X_tr)//3))
                train_ds   = SeqDataset(X_tr, y_tr_s, seq_len)
                val_ds     = SeqDataset(X_va, y_va_s, seq_len)
                train_loader = DataLoader(train_ds, batch_size=params["batch_size"], shuffle=False)
                val_loader   = DataLoader(val_ds,   batch_size=params["batch_size"], shuffle=False)

                model     = LSTMAttention(X_tr.shape[1], params).to(DEVICE)
                optimizer = torch.optim.Adam(model.parameters(), lr=params["lr"])
                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)
                criterion = nn.L1Loss()

                best_loss, tr_losses, va_losses = train_one_fold(
                    model, train_loader, val_loader, optimizer, scheduler, criterion,
                    params["epochs"], fold_name, model_dir
                )

                # ì—­ìŠ¤ì¼€ì¼ MAE (ë¦¬í¬íŒ…ìš©)
                model.eval()
                preds, trues = [], []
                with torch.no_grad():
                    for Xb, yb in val_loader:
                        Xb = Xb.to(DEVICE)
                        preds.append(model(Xb).cpu().numpy())
                        trues.append(yb.numpy())
                y_pred = np.concatenate(preds)
                y_true = np.concatenate(trues)
                y_pred_inv = y_scaler.inverse_transform(y_pred.reshape(-1,1)).ravel()
                y_true_inv = y_scaler.inverse_transform(y_true.reshape(-1,1)).ravel()
                mae = mean_absolute_error(y_true_inv, y_pred_inv)
                fold_mae.append(mae)

                # foldë³„ loss ê¸°ë¡
                fold_results[fold_name] = {"train_loss": tr_losses, "val_loss": va_losses}

                # ë§ˆì§€ë§‰ fold ê¸°ì¤€ìœ¼ë¡œ í˜„ì¬ ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥(ê°„ë‹¨í™”)
                torch.save(model.state_dict(), os.path.join(model_dir, "best_model.pt"))
                joblib.dump(x_scaler, os.path.join(model_dir, "scaler_x.pkl"))
                joblib.dump(y_scaler, os.path.join(model_dir, "scaler_y.pkl"))

            mean_mae = float(np.mean(fold_mae))
            mlflow.log_metric("mean_mae", mean_mae)

            # foldë³„ ì†ì‹¤ ê³¡ì„  ì €ì¥
            with open(os.path.join(model_dir, "losses.json"), "w", encoding="utf-8") as f:
                json.dump(fold_results, f, indent=2, ensure_ascii=False)

        return mean_mae

    return objective

# ==========================================================
# ì‹¤í–‰ (ì™¸ë¶€ run 1ê°œ + ë‚´ë¶€ nested run)
# ==========================================================
if __name__ == "__main__":
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    train = preprocess_dataframe(pd.read_csv(train_path))
    ALL_NUM = train.select_dtypes(include=[np.number]).columns.tolist()
    feature_pool = [c for c in ALL_NUM if c not in TARGETS + ["ì „ê¸°ìš”ê¸ˆ(ì›)"]]

    # ì™¸ë¶€ run 1ê°œë¡œ ì „ì²´ Stage1ì„ ê°ì‹¸ê³ , íƒœê·¸ëŠ” ì—¬ê¸°ì„œ ì„¤ì •
    with mlflow.start_run(run_name="Stage1_Full_Run"):
        mlflow.set_tag("git_commit", get_git_commit())
        mlflow.set_tag("device", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")
        mlflow.set_tag("script_path", __file__)

        results = {}
        for tgt in TARGETS:
            print(f"\n================== Target: {tgt} ==================")
            study = optuna.create_study(direction="minimize")
            study.optimize(make_objective(tgt), n_trials=N_TRIALS, show_progress_bar=True)
            results[tgt] = {"best_mae": study.best_value, "params": study.best_trial.params}
            print(f"ğŸ¯ {tgt} | Best MAE: {study.best_value:.4f}")
            print(f"ğŸ§© Params: {study.best_trial.params}")
            # ğŸ”¹ í•œê¸€ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì•ˆì „í•˜ê²Œ ë³€í™˜
            safe_name = re.sub(r"[^a-zA-Z0-9_.\-/ ]", "_", tgt)

            # ğŸ”¹ metric (ìˆ˜ì¹˜) ì €ì¥
            mlflow.log_metric(f"{safe_name}_best_mae", study.best_value)

            # ğŸ”¹ params (ë”•ì…”ë„ˆë¦¬) ì €ì¥ â€” ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ íƒœê·¸ì— ì €ì¥
            mlflow.set_tag(f"{safe_name}_best_params", str(study.best_trial.params))


        with open(os.path.join(SAVE_ROOT, "metrics.json"), "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print("\nâœ… ëª¨ë“  Stage1 ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {SAVE_ROOT}")

###################################################################################################
# # ==========================================================
# # âš¡ stage1_model_predict.py
# # âœ… Stage1 í•™ìŠµëœ ëª¨ë¸ë¡œ testì…‹ ì˜ˆì¸¡ â†’ test_with_stage1.csv ì €ì¥
# # ==========================================================
# import os, json, joblib, torch
# import numpy as np
# import pandas as pd
# from tqdm import tqdm
# from sklearn.preprocessing import StandardScaler
# from torch.utils.data import Dataset, DataLoader
# from stage1_model_train_autolog import LSTMAttention, SeqDataset, preprocess_dataframe, TARGETS, DEVICE

# # ----------------------------------------------------------
# # ê²½ë¡œ ì„¤ì •
# # ----------------------------------------------------------
# BASE_DIR  = os.path.dirname(os.path.abspath(__file__))
# DATA_DIR  = os.path.join(BASE_DIR, "../data")
# MODEL_DIR = os.path.join(BASE_DIR, "stage1_model")

# test_path = os.path.join(DATA_DIR, "fixed_test_weather_full.csv")
# save_path = os.path.join(DATA_DIR, "fixed_test_stage1_pred.csv")

# # ----------------------------------------------------------
# # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# # ----------------------------------------------------------
# print("ğŸ“‚ test ë°ì´í„° ë¡œë“œ ì¤‘...")
# test_df = preprocess_dataframe(pd.read_csv(test_path))
# print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ | shape = {test_df.shape}")

# # ----------------------------------------------------------
# # ìˆ«ìí˜• feature pool êµ¬ì„±
# # ----------------------------------------------------------
# ALL_NUM = test_df.select_dtypes(include=[np.number]).columns.tolist()
# feature_pool = [c for c in ALL_NUM if c not in TARGETS + ["ì „ê¸°ìš”ê¸ˆ(ì›)"]]
# X_full = test_df[feature_pool].values.astype(float)

# # ----------------------------------------------------------
# # ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜
# # ----------------------------------------------------------
# class SimpleDataset(Dataset):
#     def __init__(self, X, seq_len):
#         self.X, self.seq_len = X, seq_len
#     def __len__(self):
#         return len(self.X) - self.seq_len
#     def __getitem__(self, idx):
#         return torch.tensor(self.X[idx:idx+self.seq_len], dtype=torch.float32)

# # ----------------------------------------------------------
# # íƒ€ê¹ƒë³„ ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡
# # ----------------------------------------------------------
# for i, target in enumerate(TARGETS, 1):
#     model_dir = os.path.join(MODEL_DIR, f"model_{i}")
#     if not os.path.exists(model_dir):
#         print(f"âš ï¸ {target} ëª¨ë¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {model_dir}")
#         continue

#     print(f"\nğŸ” [{i}/{len(TARGETS)}] {target} ì˜ˆì¸¡ ì‹œì‘...")

#     # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
#     x_scaler = joblib.load(os.path.join(model_dir, "scaler_x.pkl"))
#     y_scaler = joblib.load(os.path.join(model_dir, "scaler_y.pkl"))

#     # feature ìŠ¤ì¼€ì¼ë§
#     X_scaled = x_scaler.transform(X_full)

#     # ì†ì‹¤ ì •ë³´ ë¡œë“œ â†’ seq_len ì¶”ì¶œ
#     try:
#         with open(os.path.join(model_dir, "losses.json"), "r", encoding="utf-8") as f:
#             losses = json.load(f)
#         # foldë³„ seq_lenì€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°€ì¥ ì§§ì€ ê°’ ì‚¬ìš©
#         seq_len = 12
#     except:
#         seq_len = 12

#     # ëª¨ë¸ íŒŒë¼ë¯¸í„° ë³µì›
#     with open(os.path.join(MODEL_DIR, "metrics.json"), "r", encoding="utf-8") as f:
#         meta = json.load(f)
#     params = meta[target]["params"]

#     # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
#     model = LSTMAttention(X_scaled.shape[1], params).to(DEVICE)
#     model.load_state_dict(torch.load(os.path.join(model_dir, "best_model.pt"), map_location=DEVICE))
#     model.eval()

#     # ì‹œí€€ìŠ¤ ê¸°ë°˜ ì˜ˆì¸¡
#     ds = SimpleDataset(X_scaled, seq_len)
#     dl = DataLoader(ds, batch_size=params["batch_size"], shuffle=False)

#     preds = []
#     with torch.no_grad():
#         for Xb in tqdm(dl, desc=f"{target} predicting"):
#             Xb = Xb.to(DEVICE)
#             y_pred = model(Xb).cpu().numpy()
#             preds.append(y_pred)

#     preds = np.concatenate(preds).ravel()
#     preds_inv = y_scaler.inverse_transform(preds.reshape(-1, 1)).ravel()

#     # ê¸¸ì´ ë§ì¶”ê¸°: seq_len offset ë§Œí¼ ì•ì— NaN
#     pred_full = np.concatenate([np.full(seq_len, np.nan), preds_inv])

#     # test_dfì— ì»¬ëŸ¼ ì¶”ê°€
#     test_df[f"{target}_pred"] = pred_full

#     print(f"âœ… {target} ì˜ˆì¸¡ ì™„ë£Œ | ì˜ˆì¸¡ì¹˜ {np.isnan(pred_full).sum()}ê°œ NaN, shape={len(pred_full)}")

# # ----------------------------------------------------------
# # ê²°ê³¼ ì €ì¥
# # ----------------------------------------------------------
# test_df.to_csv(save_path, index=False, encoding="utf-8-sig")
# print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path}")
